{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian estimation algorithm for the phase and the visibilities in a q-plates system\n",
    "\n",
    "This code is meant to be run on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from scipy.stats import bootstrap\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "gpu = torch.device(\"cuda:0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mean(batchsize, weights, particles, nparameters):\n",
    "    \"\"\"\n",
    "    Mean of the positions of the particles\n",
    "    weights.shape -> (batchsize, nparticles)\n",
    "    particles.shape -> (batchsize, nparticles, nparameters)\n",
    "    return (batchsize, nparameters)\n",
    "    \"\"\"\n",
    "    #we distinguish the theta parameter from the others\n",
    "    #* is the element-wise multiplication\n",
    "    mean_theta = torch.remainder(torch.angle(torch.sum((torch.exp(1.0j*particles[:, :, 0:1])*weights[:, :, None]), dim=1)), 2*math.pi) #(shape -> batchsize)\n",
    "    #the above exponentials in the weighted mean are necessary because theta is a circular variable\n",
    "    #weighted mean values of the visibilities\n",
    "    mean_visibilities = torch.sum(particles[:, :, 1:nparameters]*weights[:, :, None], dim=1, dtype=torch.double) #shape -> batchsize*nparameters-1\n",
    "    #returns the concatenated arrays\n",
    "    return torch.cat((mean_theta, mean_visibilities), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_covariance(batchsize, weights, particles, nparticles, nparameters):\n",
    "    \"\"\"\n",
    "    Computes the covariant matrix\n",
    "    weights.shape -> (batchsize, nparticles)\n",
    "    particles.shape -> (batchsize, nparticles, nparameters)\n",
    "    \"\"\"\n",
    "    #broadcasted version on the mean to all the particles in a batch\n",
    "    broadcasted_mean = torch.broadcast_to(estimate_mean(batchsize, weights, particles, nparameters)[:, None, :], (batchsize, nparticles, nparameters))\n",
    "    #broadcasted weights to all the parameters\n",
    "    broadcasted_weights = torch.broadcast_to(weights[:, :, None], (batchsize, nparticles, nparameters))\n",
    "    #the difference between the particle value and the angula mean has to be computed with the circular distance\n",
    "    diff_theta = math.pi-torch.abs(torch.remainder(particles[:, :, 0:1] - broadcasted_mean[:, :, 0:1], 2*math.pi)-math.pi)\n",
    "    #difference of the particles visibilities and their mean\n",
    "    diff_visibilities = particles[:, :, 1:nparameters] - broadcasted_mean[:, :, 1:nparameters]\n",
    "    diff = torch.cat((diff_theta, diff_visibilities), dim=2)\n",
    "    #this matrix product return the estimated covariance matrix\n",
    "    return torch.transpose(broadcasted_weights*diff, 1, 2)@diff         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_outcomes(batchsize, outcomes, particles, qplates, phase, qresources, nparticles, gpu):\n",
    "    \"\"\"\n",
    "    Probability of measurement\n",
    "    outcomes.shape -> (batchsize, 1)\n",
    "    particles.shape -> (batchsize, nparticles, nparameters)\n",
    "    qplates.shape -> (batchsize, 1)\n",
    "    phase.shape -> (batchsize, 1)\n",
    "    qresources.shape -> (nparameters-1, )\n",
    "    the number of q-plates available is the number of parameter we are estimating - 1 (the phase)\n",
    "    return a tensor of size (batchsize, nparticles), containing the probabilities of outcome for each of the particles\n",
    "    \"\"\"\n",
    "    #define broadcasted versions of outcomes, measurement, and qplates\n",
    "    outcomes_broad = torch.broadcast_to(outcomes, (batchsize, nparticles))\n",
    "    phase_broad = torch.broadcast_to(phase, (batchsize, nparticles))\n",
    "    qplates_broad = torch.broadcast_to(qplates, (batchsize, nparticles))\n",
    "    #the qplate numer is used to select the correct visibility\n",
    "    visibilities = torch.take_along_dim(particles, qplates_broad.long()[:, :, None], dim = 2)[:, :, 0]\n",
    "    #here we define the tensors for the result and for the phase\n",
    "    out_set = torch.tensor([-1.0, +1.0], dtype=torch.double, device=gpu)\n",
    "    phase_set = torch.tensor([0.0, math.pi/2], dtype=torch.double, device=gpu)\n",
    "    #return the tensor contain the probability of having observed outcomes with the given measurements for each particle.\n",
    "    return (1.0+out_set[outcomes_broad.long()]*visibilities*torch.cos(particles[:, :, 0]*qresources[(qplates_broad-1).long()]+phase_set[phase_broad.long()]))/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_update(batchsize, weights, outcomes, particles, qplates, phase, qresources, nparticles):\n",
    "    \"\"\"\n",
    "    Returns the updated weights, applies the Bayesian rule\n",
    "    \"\"\"\n",
    "    tmp_weights = prob_outcomes(batchsize, outcomes, particles, qplates, phase, qresources, nparticles, gpu)*weights\n",
    "    new_weights = tmp_weights/torch.sum(tmp_weights, dim=1)[:, None]\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_simplified(batchsize, weights, particles, nparticles, nparameters, a, gpu):\n",
    "    \"\"\"\n",
    "    Simplified resampling\n",
    "    it produce as output\n",
    "    newParticles.shape -> (batchsize, nparticles, nparameters)\n",
    "    \"\"\"\n",
    "    #sampled particles\n",
    "    pos = torch.multinomial(weights, nparticles, replacement=True)\n",
    "    #we need to broadcast the selection along the dimention of the parameters, this is necessary in order to apply the follwoing function\n",
    "    pos_broadcasted = torch.broadcast_to(pos[:, :, None], (batchsize, nparticles, nparameters))\n",
    "    #this filters the tensor particles, in the second direction according to the dimension 1\n",
    "    new_particles = torch.take_along_dim(particles, pos_broadcasted, dim=1)\n",
    "    new_weights = torch.tensor(np.full((batchsize, nparticles), 1/nparticles), dtype=torch.double, device=gpu)\n",
    "    return new_particles, new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(batchsize, weights, particles, nparticles, nparameters, a, gpu):\n",
    "    \"\"\"\n",
    "    Circular resampling algorithm with concentration towards the mean\n",
    "    \"\"\"\n",
    "    mean = estimate_mean(batchsize, weights, particles, nparameters)\n",
    "    h = math.sqrt(1-a**2)\n",
    "    reduced_cov = (h**2)*estimate_covariance(batchsize, weights, particles, nparticles, nparameters)\n",
    "    #sampled particles\n",
    "    pos = torch.multinomial(weights, nparticles, replacement=True)\n",
    "    #we need to broadcast the selection along the dimention of the parameters, this is necessary in order to apply the follwoing function\n",
    "    pos_broadcasted = torch.broadcast_to(pos[:, :, None], (batchsize, nparticles, nparameters))\n",
    "    #this filters the tensor particles, in the second direction according to the dimension 1\n",
    "    new_particles = torch.take_along_dim(particles, pos_broadcasted, dim=1)\n",
    "    #this new particles should be now mixed with a broadcasted version of the mean\n",
    "    broadcasted_mean = torch.broadcast_to(estimate_mean(batchsize, weights, particles, nparameters)[:, None, :], (batchsize, nparticles, nparameters))\n",
    "    mean_gaussian = torch.zeros(batchsize, nparticles, nparameters, dtype=torch.double, device=gpu)\n",
    "    #linear combination of the visibilitie\n",
    "    mean_gaussian[:, :, 1:nparameters] = a*new_particles[:, :, 1:nparameters] + (1-a)*broadcasted_mean[:, :, 1:nparameters]\n",
    "    #linear combinations of the angles\n",
    "    mean_gaussian[:, :, 0] = torch.remainder(torch.angle(a*torch.exp(1.0j*new_particles[:, :, 0]) + (1-a)*torch.exp(1.0j*broadcasted_mean[:, :, 0])), 2*math.pi)\n",
    "    #now we sample from a multivariate Gaussian with mean contained in meanGaussian, and covariance matrix the brodcasted version of reducedCov\n",
    "    reduced_cov_broad = torch.broadcast_to(reduced_cov[:, None, :, :], (batchsize, nparticles, nparameters, nparameters))\n",
    "    #new particles\n",
    "    normal = MultivariateNormal(mean_gaussian, reduced_cov_broad)\n",
    "    #the angles have to be casted in [0, 2*pi] and the visibilities in [0, 1]\n",
    "    new_particles = normal.sample()\n",
    "    new_particles[:, :, 0] = torch.remainder(new_particles[:, :, 0], 2*math.pi)\n",
    "    #how can we broadcast the operations min and max? We can use a mask\n",
    "    pos_over_1 = (new_particles[:, :, 1:nparameters] > 1.0).long()\n",
    "    new_particles[:, :, 1:nparameters] = new_particles[:, :, 1:nparameters]*(1-pos_over_1)+pos_over_1\n",
    "    pos_under_0 = (new_particles[:, :, 1:nparameters] < 0.0).long()\n",
    "    new_particles[:, :, 1:nparameters] = new_particles[:, :, 1:nparameters]*(1-pos_under_0)\n",
    "    new_weights = torch.tensor(np.full((batchsize, nparticles), 1/nparticles), dtype=torch.double, device=gpu)\n",
    "    return new_particles, new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(batchsize, weights, particles, qplates, phase, Q, nparticles, nparameters, qresources, gpu):\n",
    "    \"\"\"\n",
    "    Function that compute the utility function of the list of measurements defined by the tensors qplates and phase\n",
    "    weights.shape -> (batchsize, nparticles)\n",
    "    particles.shape -> (batchsize, nparticles, nparameters)\n",
    "    qplates.shape -> (batchsize, 1)\n",
    "    phase.shape -> (batchsize, 1)\n",
    "    Q.shape -> (nparameters, nparameters)\n",
    "    \"\"\"\n",
    "    u_outcomes = torch.zeros(batchsize, 2, dtype=torch.double, device=gpu)\n",
    "    for out in range(2):\n",
    "        #broadcasted version of the output\n",
    "        out_broad = torch.broadcast_to(torch.tensor([out], device=gpu), size=(batchsize, 1))\n",
    "        #broadcasted version of the weight matrix\n",
    "        Q_broad = torch.broadcast_to(Q[None, :, :], size=(batchsize, nparameters, nparameters))\n",
    "        hyp_weights = bayesian_update(batchsize, weights, out_broad, particles, qplates, phase, qresources, nparticles)\n",
    "        hyp_covariance = estimate_covariance(batchsize, hyp_weights, particles, nparticles, nparameters)\n",
    "        hyp_nvariance = -((torch.diagonal(hyp_covariance@Q_broad, dim1=1, dim2=2)).sum(dim=1))\n",
    "        prob_out = prob_outcomes(batchsize, out_broad, particles, qplates, phase, qresources, nparticles, gpu)\n",
    "        u_outcomes[:, out] = hyp_nvariance*torch.diagonal(weights@(torch.transpose(prob_out, 0, 1)), dim1=0, dim2=1)\n",
    "    return torch.sum(u_outcomes, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(batchsize, weights, particles, nparticles, nparameters, Q, qresources, gpu):\n",
    "    \"\"\"\n",
    "    Function that choses the optimal measurements, according to the utility function\n",
    "    \"\"\"\n",
    "    #in this tensor we save the utility for each of the possible measurements, and each of the estimation in the batch\n",
    "    utility_tensor = torch.zeros(batchsize, 2*qresources.shape[0], dtype=torch.double, device=gpu)\n",
    "    #qp = 0, 1, 2, 3\n",
    "    for qp in range(qresources.shape[0]):\n",
    "        #r = 0, 1\n",
    "        for p in range(2):\n",
    "            qplates = torch.tensor(np.full((batchsize, 1), qp+1), device=gpu)\n",
    "            phase = torch.tensor(np.full((batchsize, 1), p), device=gpu)\n",
    "            utility_tensor[:, 2*qp+p] = utility(batchsize, weights, particles, qplates, phase, Q, nparticles, nparameters, qresources, gpu)\n",
    "    #take the argmax of utilityTensor in dim=1\n",
    "    optimal_meas = torch.argmax(utility_tensor, axis=1)\n",
    "    phase = torch.remainder(optimal_meas, 2)\n",
    "    qplates = (optimal_meas-phase)/2+1\n",
    "    return qplates[:, None], phase[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(batchsize, qplates, phase, trueangles, visibilities, qresources, gpu):\n",
    "    \"\"\"\n",
    "    Simulation of the experiment. \n",
    "    \"\"\"\n",
    "    #computation of the probability of the 0 outcome, corrisponding to -1\n",
    "    #here we define the tensors for the result and for the phase\n",
    "    out_set = torch.tensor([-1.0, +1.0], dtype=torch.double, device=gpu)\n",
    "    phase_set = torch.tensor([0.0, math.pi/2], dtype=torch.double, device=gpu)\n",
    "    #probability of having observed outcomes with the given measurements for each particle.\n",
    "    prob_out_0 = (1.0-1.0*visibilities[(qplates-1).long()]*torch.cos(trueangles*qresources[(qplates-1).long()]+phase_set[phase.long()]))/2.0\n",
    "    p = torch.rand(batchsize, 1, device=gpu)\n",
    "    #true if outcome 0 happens\n",
    "    outcomes = p<prob_out_0\n",
    "    outcomes = 1 - outcomes.int()\n",
    "    return outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_estimation(batchsize, nparticles, nparameters, a, resample_threshold, Q, trueangles, visibilities, qresources, nsteps, gpu):\n",
    "    \"\"\"\n",
    "    This function runs an instance of the estimation procedure\n",
    "    \"\"\"\n",
    "    #the weight are uniform at the beginning\n",
    "    weights = torch.tensor(np.full((batchsize, nparticles), 1/nparticles), dtype=torch.double, device=gpu)\n",
    "    #the particles are initialized at random\n",
    "    particles = torch.rand(batchsize, nparticles, nparameters, dtype=torch.double, device=gpu)\n",
    "    particles[:, :, 0] = 2*math.pi*particles[:, :, 0]\n",
    "    #initial measurement\n",
    "    qplates = torch.tensor(np.full((batchsize, 1), 1), device=gpu)\n",
    "    phase = torch.tensor(np.full((batchsize, 1), 0), device=gpu)\n",
    "    #number of photon used in the estimation\n",
    "    for i in range(nsteps):\n",
    "        #simulation of the esperiment\n",
    "        outcomes = simulation(batchsize, qplates, phase, trueangles, visibilities, qresources, gpu)\n",
    "        #Bayesian update of the weights of the particle filter\n",
    "        weights = bayesian_update(batchsize, weights, outcomes, particles, qplates, phase, qresources, nparticles)\n",
    "        #computation of the next optimal measurement in a \"greedy\" fashion\n",
    "        qplates, phase = optimize(batchsize, weights, particles, nparticles, nparameters, Q, qresources, gpu)\n",
    "        #checking the condition for resampling\n",
    "        #true means we need to resample the simulation, which corresponds to 1, and therefore gets counter among the non-null values\n",
    "        array_resample = (1/torch.sum(weights**2, dim=1) < resample_threshold*nparticles).long()\n",
    "        #to_resample -> (batch_resample, ) -> position of the experiments to be resampled\n",
    "        to_resample = torch.nonzero(array_resample)[:, 0]\n",
    "        #number of simulations to resamples\n",
    "        batch_resample = to_resample.size(0)\n",
    "        #if there are any\n",
    "        if (batch_resample > 0):\n",
    "            #broadcasting of the index to filter the weights\n",
    "            to_resample_broad_1 = torch.broadcast_to(to_resample[:, None], (batch_resample, nparticles))\n",
    "            #broadcasting to filter the particles\n",
    "            to_resample_broad_2 = torch.broadcast_to(to_resample[:, None, None], (batch_resample, nparticles, nparameters))\n",
    "            #experiments to resample\n",
    "            resample_weights = torch.take_along_dim(weights, to_resample_broad_1, dim=0)\n",
    "            resample_particles = torch.take_along_dim(particles, to_resample_broad_2, dim=0)\n",
    "            #resampling\n",
    "            resample_particles, resample_weights = resample_simplified(batch_resample, resample_weights, resample_particles, nparticles, nparameters, a, gpu)\n",
    "            #inserting the new values back in particles and weights\n",
    "            linear_index = torch.arange(0, batch_resample, 1, device=gpu)\n",
    "            particles[to_resample.long(), :, :] = resample_particles[linear_index.long(), :, :]\n",
    "            weights[to_resample.long(), :] = resample_weights[linear_index.long(), :]\n",
    "        #progressing of the estimation\n",
    "        #print((np.round(i/nsteps*100)).astype(int), \"%\")\n",
    "        #clear_output(wait=True)\n",
    "   \n",
    "    return estimate_mean(batchsize, weights, particles, nparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_estimation_resources(batchsize, nparticles, nparameters, a, resample_threshold, Q, trueangles, visibilities, qresources, max_resources, gpu):\n",
    "    \"\"\"\n",
    "    This function runs an instance of the estimation procedure and keeps track of the resources used\n",
    "    \"\"\"\n",
    "    #the weight are uniform at the beginning\n",
    "    weights = torch.tensor(np.full((batchsize, nparticles), 1/nparticles), dtype=torch.torch.double, device=gpu)\n",
    "    #the particles are initialized at random\n",
    "    particles = torch.rand(batchsize, nparticles, nparameters, dtype=torch.double, device=gpu)\n",
    "    particles[:, :, 0] = 2*math.pi*particles[:, :, 0]\n",
    "    #initial measurement\n",
    "    qplates = torch.tensor(np.full((batchsize, 1), 1), device=gpu)\n",
    "    phase = torch.tensor(np.full((batchsize, 1), 0), device=gpu)\n",
    "    #the following array is of shape (batchsize, max_resources, nparameters) and is meant to contain the results of the estimations\n",
    "    #we keep track of the current number of resources used in total for each back and use it as indeces\n",
    "    results_resources = torch.zeros(batchsize, max_resources, nparameters, dtype=torch.double, device=gpu)\n",
    "    #total number of resources used\n",
    "    current_resources = torch.tensor(np.full(batchsize, 0), dtype=torch.long, device=gpu)\n",
    "    selected_resources = torch.zeros(batchsize, max_resources, 4, dtype=torch.double, device=gpu)\n",
    "    #the looping is stopped only when the first batch reaches max_resources\n",
    "    #with this condition we stop too early we ask for the median to reach max_resources, but then we need to discard the batches that have gone too far.\n",
    "    print(\"Estimating...\")\n",
    "    #while (torch.median(current_resources) < max_resources):\n",
    "    #while(((current_resources > max_resources).long()).sum(dim=0) < 0.99*batchsize):\n",
    "    while(torch.quantile(current_resources.float(), 0.01) < max_resources):\n",
    "        #simulation of the esperiment\n",
    "        outcomes = simulation(batchsize, qplates, phase, trueangles, visibilities, qresources, gpu)\n",
    "        #Bayesian update of the weights of the particle filter\n",
    "        weights = bayesian_update(batchsize, weights, outcomes, particles, qplates, phase, qresources, nparticles)\n",
    "        #computation of the next optimal measurement in a \"greedy\" fashion\n",
    "        qplates, phase = optimize(batchsize, weights, particles, nparticles, nparameters, Q, qresources, gpu)\n",
    "        #updating the number of resources\n",
    "        current_resources += qresources[(qplates-1).long()[:, 0]]\n",
    "        #saving the partial result at the current value of the total number of resources\n",
    "        #in this line we go out of index with the while condition we chose\n",
    "        #results_resources[torch.arange(0, batchsize, 1), current_resources.long() - torch.remainder(current_resources.long(), delta_n)+deltan/2, :]\n",
    "        #we select the batches that have't gone too far\n",
    "        #with this masking trick the results of the simulations that exceed max_resources are stored in position resources = 0, and are eliminated at the end\n",
    "        mask = (current_resources < max_resources).long()\n",
    "        masked_resources = mask*current_resources\n",
    "        results_resources[torch.arange(0, batchsize, 1), masked_resources.long(), :] = estimate_mean(batchsize, weights, particles, nparameters)\n",
    "        selected_resources[torch.arange(0, batchsize, 1), masked_resources.long(), (qplates-1).long()[:, 0]] += 1\n",
    "        #checking the condition for resampling\n",
    "        #true means we need to resample the simulation, which corresponds to 1, and therefore gets counter among the non-null values\n",
    "        array_resample = (1/torch.sum(weights**2, dim=1) < resample_threshold*nparticles).long()\n",
    "        #to_resample -> (batch_resample, ) -> position of the experiments to be resampled\n",
    "        to_resample = torch.nonzero(array_resample)[:, 0]\n",
    "        #number of simulations to resamples\n",
    "        batch_resample = to_resample.size(0)\n",
    "        #if there are any\n",
    "        if (batch_resample > 0):\n",
    "            #broadcasting of the index to filter the weights\n",
    "            to_resample_broad_1 = torch.broadcast_to(to_resample[:, None], (batch_resample, nparticles))\n",
    "            #broadcasting to filter the particles\n",
    "            to_resample_broad_2 = torch.broadcast_to(to_resample[:, None, None], (batch_resample, nparticles, nparameters))\n",
    "            #experiments to resample\n",
    "            resample_weights = torch.take_along_dim(weights, to_resample_broad_1, dim=0)\n",
    "            resample_particles = torch.take_along_dim(particles, to_resample_broad_2, dim=0)\n",
    "            #resampling\n",
    "            resample_particles, resample_weights = resample(batch_resample, resample_weights, resample_particles, nparticles, nparameters, a, gpu)\n",
    "            #inserting the new values back in particles and weights\n",
    "            linear_index = torch.arange(0, batch_resample, 1, device=gpu)\n",
    "            particles[to_resample.long(), :, :] = resample_particles[linear_index.long(), :, :]\n",
    "            weights[to_resample.long(), :] = resample_weights[linear_index.long(), :]\n",
    "        #progressing of the estimation\n",
    "        #print(torch.round(100*((current_resources > max_resources).long()).sum(dim=0)/(0.9*batchsize)), \"%\")\n",
    "        #torch.quantile(current_resources, 0.05) > max_resources\n",
    "        #print(\"1% quantile of the used resources:\", int(torch.quantile(current_resources.float(), 0.01).item()))\n",
    "        #clear_output(wait=True)\n",
    "    print(\"Done.\")\n",
    "    results_resources[:, 0, :] = 0\n",
    "    return results_resources, current_resources, selected_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization_filter(batch, particles, weights, nparticles, nparameters):\n",
    "    \"\"\"\n",
    "    Visualization of the phase distribution represented by the particle filter\n",
    "    \"\"\"\n",
    "    lattice_step = 100\n",
    "    hist_values = torch.zeros(lattice_step).cpu()\n",
    "    particles_cpu = particles.cpu()\n",
    "    weights_cpu = weights.cpu()\n",
    "    for i in range(nparticles):\n",
    "        hist_values[torch.floor(lattice_step*(particles_cpu[batch, i, 0]/(2*math.pi))).long()] += weights_cpu[batch, i]\n",
    "    plt.plot(torch.arange(0, 2*math.pi, 2*math.pi/lattice_step).cpu(), hist_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_error(scalar_error, non_zero_mask, new_batchsize):\n",
    "    \"\"\"\n",
    "    This function filters away the resource number that do not correspond to any experiment and resamples the result\n",
    "    \"\"\"\n",
    "    #here a threshold conditions could be inserted\n",
    "    non_null_resources = torch.nonzero((non_zero_mask.sum(dim=0) > 0).long())[:, 0]\n",
    "    #filtering of the resources and of the mask\n",
    "    filtered_error = torch.take_along_dim(scalar_error, non_null_resources[None, :], dim=1)\n",
    "    filtered_mask = torch.take_along_dim(non_zero_mask, non_null_resources[None, :], dim=1)\n",
    "    #resampling of the exprimental results\n",
    "    resampled_positions = torch.transpose(torch.multinomial(torch.transpose(filtered_mask, 0, 1).double(), new_batchsize, replacement=True), 0, 1)\n",
    "    full_results = torch.take_along_dim(filtered_error, resampled_positions, dim=0)\n",
    "    return full_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In line diff_results[:, :, 0, 0] = diff_results[:, :, 0, 0]/2 we divided by two the error on the phase because we are actually interested in the error on $\\theta$, since we are passing $2\\theta$ as true phase. The errors on the visibilities should NOT be similarly halfed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation(batchsize, nparticles, nparameters, a, resample_threshold, Q, Qout, phase, visibilities, qresources, max_resources, cluster_size, cluster_cut, new_batchsize, gpu):\n",
    "    \"\"\"\n",
    "    This function perform the simulation and produces a polished version of the outcome, that is the error as a function of the number of employed resources \n",
    "    \"\"\"\n",
    "    #trueangles, index = torch.sort(2*math.pi*torch.rand(batchsize, 1, dtype=torch.double, device=gpu), dim=0)\n",
    "    #batch of true angles, all equal to phase\n",
    "    trueangles = phase*torch.ones(batchsize, 1, dtype=torch.double, device=gpu)\n",
    "    #mean = run_estimation(batchsize, nparticles, nparameters, a,resample_threshold, Q, trueangles, qresources, 200, gpu)\n",
    "    results_resources, current_resources, _ = run_estimation_resources(batchsize, nparticles, nparameters, a, resample_threshold, Q, trueangles, visibilities, \n",
    "                                                                    qresources, max_resources, gpu)\n",
    "    #it is sufficient to check the phase to be zero (the estimated visibilities will also be zero)\n",
    "    #we assume of course that exactly 0 is not a return value of the algorithm   \n",
    "    non_zero_mask = (1-(results_resources == 0).long())[:, :, 0]\n",
    "    #computation of the scalar error\n",
    "    visibilities = torch.tensor([0.900, 0.875, 0.850, 0.765], dtype=torch.double, device=gpu)\n",
    "    vis_broad = torch.broadcast_to(visibilities, (batchsize, nparameters-1))\n",
    "    true_param = torch.cat((trueangles, vis_broad), dim=1)\n",
    "    diff_results = results_resources[:, :, :, None] - true_param[:, None, :, None]\n",
    "    #the difference in the theta parameter must be corrected to account for the circular nature of the parameter\n",
    "    diff_results[:, :, 0, 0] = math.pi-torch.abs(torch.remainder(diff_results[:, :, 0, 0], 2*math.pi)-math.pi)\n",
    "    #Achtung: this line mist be added to account for the fact that we are estimation 2*theta but we are interested in the error on theta\n",
    "    diff_results[:, :, 0, 0] = diff_results[:, :, 0, 0]/2\n",
    "    #this scalar error is meaningful only for the non zero values, indeed we filter it accordingly\n",
    "    scalar_error = (torch.transpose(diff_results, 2, 3)@Qout[None, None, :, :]@diff_results)[:, :, 0, 0]\n",
    "    #filtering of the scalar error\n",
    "    scalar_error = non_zero_mask*scalar_error\n",
    "    #the scalar error needs to be clustered\n",
    "    #we group the values in (batch, 30000) in groups of 20 each\n",
    "    #for the data analysis to work cluster_cut should be a multiple of cluster_size, and max_resources should also be a multiple of cluster_size\n",
    "    num_step = int(max_resources/cluster_size)\n",
    "    scalar_error_clustered = torch.zeros(batchsize*cluster_size, num_step, device=gpu)\n",
    "    non_zero_mask_clustered = torch.zeros(batchsize*cluster_size, num_step, device=gpu)\n",
    "    new_resources = torch.arange(cluster_size/2, max_resources, cluster_size)\n",
    "    for j in range(num_step):\n",
    "        scalar_error_clustered[:, j] = torch.reshape(scalar_error[:, (j*cluster_size):((j+1)*cluster_size)], (batchsize*cluster_size, ))\n",
    "        non_zero_mask_clustered[:, j] = torch.reshape(non_zero_mask[:, (j*cluster_size):((j+1)*cluster_size)], (batchsize*cluster_size, ))\n",
    "    effective_batchsize = int(torch.min(torch.sum(non_zero_mask_clustered, axis=0)).item())\n",
    "    #print(torch.sum(non_zero_mask_clustered, axis=0))\n",
    "    clustered_error_1 = filter_error(scalar_error[:, 0:cluster_cut], non_zero_mask[:, 0:cluster_cut], new_batchsize)\n",
    "    clustered_error_2 = filter_error(scalar_error_clustered, non_zero_mask_clustered, new_batchsize)\n",
    "    #An important sanity check is that the size of clustered_error_1 should be cluster_cut-1, and that of clustered_error_2 max_resources/cluster_size\n",
    "    if(not (clustered_error_1.shape[1] == (cluster_cut-1) and (clustered_error_2.shape[1] == int(max_resources/cluster_size)))):\n",
    "        return\n",
    "    cluster_error = torch.cat((clustered_error_1, clustered_error_2[:, int(cluster_cut/cluster_size):]), axis=1)\n",
    "    resources = torch.cat((torch.arange(1, cluster_cut, 1), new_resources[int(cluster_cut/cluster_size):]))\n",
    "    number_points = cluster_error.shape[1]\n",
    "    resources_bar = torch.zeros(number_points, dtype=torch.double, device=gpu)\n",
    "    resources_bar[cluster_cut:] = cluster_size/2\n",
    "    return resources.cpu().numpy(), resources_bar.cpu().numpy(), cluster_error.cpu().numpy(), effective_batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that the new_batchsize is not greater than the minimum number of actual measurements in a cluster of data. Otherwise we are oversamping the data and therefore the confidence interval won't be accurate. The median is chosen to limitate the importance of the outliers in the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_room(batchsize, new_batchsize, Q, Qout, max_resources):\n",
    "    \"\"\"\n",
    "    All the true parameters of the estimation, the quantum resources, \n",
    "    the visibilities and the technical parameters that make the estimation work are hidden in this function.\n",
    "    Here we also compute the bayesian bound for the media square error.\n",
    "    Q is the weights function used for the estimation, Qout is used for the visualization of the results.\n",
    "    Usually Q=Qout\n",
    "    \"\"\"\n",
    "    phases = 2*np.array([0.00235, 0.06145, 0.29345, 0.21635, 0.38000, 0.49620, 0.46280, 1.13975, \n",
    "                      1.26430, 1.44890, 1.66450, 1.75520, 1.87500, 2.12710, 2.58995, 2.74000, 2.96000])\n",
    "    num_phases = phases.size\n",
    "    #up to batchsize = 10000 should be supported by the gpu\n",
    "    #batchsize = 1000\n",
    "    #nparticles = 5000\n",
    "    nparticles = 5000\n",
    "    #the number of parameters should be one more than the \n",
    "    visibilities = torch.tensor([0.900, 0.875, 0.850, 0.765], dtype=torch.double, device=gpu)\n",
    "    qresources = torch.tensor([1, 2, 11, 51], dtype=torch.long, device=gpu)\n",
    "    nparameters = 5\n",
    "    a = 0.9\n",
    "    resample_threshold = 0.5\n",
    "    cluster_size = 50\n",
    "    cluster_cut = 100\n",
    "    #the new batchsize should be comparable with the size of the smaller clustered batch, this can be verified a posteriori\n",
    "    phases_error = np.zeros((new_batchsize, (cluster_cut-1)+int(max_resources/cluster_size)-int(cluster_cut/cluster_size)))\n",
    "    for i in range(num_phases): \n",
    "        #sum the error for diferent angles, in order to get one sample of delta_1 + delta_2 + ... delta_J\n",
    "        #both the resource_bar and the resources should be the same for every estimation\n",
    "        resources, resources_bar, cluster_error, effective_batchsize = estimation(batchsize, nparticles, nparameters, a, resample_threshold, Q, Qout, phases[i], visibilities, \n",
    "                                                                                  qresources, max_resources, cluster_size, cluster_cut, new_batchsize, gpu)\n",
    "        print(\"Phase\", i+1, \"/\", num_phases, \"completed.\")\n",
    "        print(\"Size of the smaller effective batch:\", effective_batchsize)\n",
    "        phases_error += cluster_error/num_phases\n",
    "    coeff_I_theta = np.zeros(4)\n",
    "    coeff_I_vis = np.zeros(4)\n",
    "    #this line accounts for the halfed phase\n",
    "    for i in range(4):\n",
    "        coeff_I_theta[i] = 2*qresources.cpu().numpy()[i]**2*(1-np.sqrt(1-(visibilities.cpu().numpy()[i])**2));\n",
    "        coeff_I_vis[i] = 2*(1-np.sqrt(1-(visibilities.cpu().numpy()[i])**2))/((visibilities.cpu().numpy()[i])**2*np.sqrt(1-(visibilities.cpu().numpy()[i])**2));\n",
    "    nu = cp.Variable(4)\n",
    "    van_trees_expression = (Q.cpu().numpy()[0, 0]*cp.inv_pos(cp.sum(cp.multiply(nu, coeff_I_theta)))/4+\n",
    "                            Q.cpu().numpy()[1, 1]*cp.inv_pos(cp.multiply(nu, coeff_I_vis)[0])+\n",
    "                            Q.cpu().numpy()[2, 2]*cp.inv_pos(cp.multiply(nu, coeff_I_vis)[1])+\n",
    "                            Q.cpu().numpy()[3, 3]*cp.inv_pos(cp.multiply(nu, coeff_I_vis)[2])+\n",
    "                            Q.cpu().numpy()[4, 4]*cp.inv_pos(cp.multiply(nu, coeff_I_vis)[3]))\n",
    "    objective = cp.Minimize(van_trees_expression)\n",
    "    constraints = [nu >= 0, 2*cp.sum(cp.multiply(qresources.cpu().numpy(), nu)) == 1]\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    #The optimal objective value is returned by `prob.solve()`.\n",
    "    van_trees_bound = 0.454896*prob.solve(verbose=False)\n",
    "    #now we implement the bootstrap procedure\n",
    "    res = bootstrap((phases_error, ), np.median, axis=0, confidence_level=0.99, n_resamples=1000, method='basic')\n",
    "    #median error to be plotted, this is chosen to reduce the fluactuations.\n",
    "    median = np.median(phases_error, axis=0)\n",
    "    #lower confidence interval\n",
    "    ci_l = res.confidence_interval[0]\n",
    "    #higher confidence interval\n",
    "    ci_h = res.confidence_interval[1]\n",
    "    #plot\n",
    "    plt.xlabel(\"Resources\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.plot(resources[10:], median[10:], 'b-')\n",
    "    plt.fill_between(resources[10:], ci_l[10:], ci_h[10:], color='r', alpha=.2)\n",
    "    \n",
    "    #we plot the SQL and the HS just as comparison.\n",
    "    #plt.plot(resources[10:], 1/resources[10:], 'g')\n",
    "    #plt.plot(resources[10:], 1/resources[10:]**2, 'r')\n",
    "    #plt.plot(resources[10:], van_trees_bound/resources[10:], 'k')\n",
    "    file_name = (\"estimation\" + str(int(Q.cpu().numpy()[0, 0]))+\n",
    "                 str(int(Q.cpu().numpy()[1, 1]))+\n",
    "                 str(int(Q.cpu().numpy()[2, 2]))+\n",
    "                 str(int(Q.cpu().numpy()[3, 3]))+\n",
    "                 str(int(Q.cpu().numpy()[4, 4]))+ \".svg\")\n",
    "    plt.savefig(file_name)\n",
    "    return resources, median, ci_l, ci_h, van_trees_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparameters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimation of the phase and the first visibility, but only the visibility is plot\n",
    "Q = torch.zeros(nparameters, nparameters, dtype=torch.double, device=gpu)\n",
    "Q[0, 0] = 1\n",
    "Q[1, 1] = 1\n",
    "resources, median, ci_l, ci_h, van_trees_bound = control_room(30, 100, Q, Q, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimation of the phase and the second visibility, but only the visibility is plot\n",
    "Q = torch.zeros(nparameters, nparameters, dtype=torch.double, device=gpu)\n",
    "Q[0, 0] = 1\n",
    "Q[2, 2] = 1\n",
    "resources, median, ci_l, ci_h, van_trees_bound = control_room(40, 100, Q, Q, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimation of the phase and the third visibility, but only the visibility is plot\n",
    "Q = torch.zeros(nparameters, nparameters, dtype=torch.double, device=gpu)\n",
    "Q[0, 0] = 1\n",
    "Q[3, 3] = 1\n",
    "resources, median, ci_l, ci_h, van_trees_bound = control_room(70, 100, Q, Q, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimation of the phase and the fourth visibility but only the visibility is plot\n",
    "Q = torch.zeros(nparameters, nparameters, dtype=torch.double, device=gpu)\n",
    "Q[0, 0] = 1\n",
    "Q[4, 4] = 1\n",
    "resources, median, ci_l, ci_h, van_trees_bound = control_room(100, 100, Q, Q, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating...\n"
     ]
    }
   ],
   "source": [
    "#estimation of all the parameters, but only the visibilities are plot\n",
    "Q = torch.ones(nparameters, nparameters, dtype=torch.double, device=gpu)\n",
    "control_room(100, 100, Q, Q, 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the q-plate use as a function of the resource number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This data analysis is meant to produce the array selected_resources, from which we build some interesting plots.\n",
    "batchsize = 1000\n",
    "nparticles = 5000\n",
    "nparameters = 5\n",
    "a = 0.9\n",
    "resample_threshold = 0.5\n",
    "Q = torch.zeros(nparameters, nparameters, dtype=torch.double, device=gpu)\n",
    "Q[0, 0] = 1\n",
    "phase_number = 1\n",
    "visibilities = torch.tensor([0.900, 0.875, 0.850, 0.765], dtype=torch.double, device=gpu)\n",
    "qresources = torch.tensor([1, 2, 11, 51], dtype=torch.long, device=gpu)\n",
    "max_resources = 30000\n",
    "simulation_bool = False\n",
    "\n",
    "selected_resources_sum = torch.zeros(batchsize, max_resources, 4, dtype=torch.double, device=gpu)\n",
    "\n",
    "phases_list = np.array([2.35000e-03, 6.14500e-02, 3.80000e-01, 4.96200e-01, 1.66450e+00, 1.87500e+00, 2.58995e+00, 2.96000e+00])\n",
    "for i in range(8):\n",
    "    trueangles = phases_list[i]*torch.ones(batchsize, 1, dtype=torch.double, device=gpu)\n",
    "    results_resources, _, selected_resources = run_estimation_resources(batchsize, nparticles, nparameters, a, resample_threshold, Q, trueangles, visibilities, qresources, max_resources, gpu)\n",
    "    selected_resources_sum += selected_resources\n",
    "\n",
    "summed = torch.sum(selected_resources_sum, dim=0)\n",
    "#the y-axis is the number of uses of a certain q-plate as a function of the total number of resources\n",
    "plt.plot(summed[1:, 0].cpu().numpy())\n",
    "plt.plot(summed[1:, 1].cpu().numpy())\n",
    "plt.plot(summed[1:, 2].cpu().numpy())\n",
    "plt.plot(summed[1:, 3].cpu().numpy())\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig(\"frequency10000.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
